{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Reminder: This üìò `Python` notebook can be run from VS Code with [these prerequisites](../PREREQS.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use this notebook: \n",
    "\n",
    "* Just read the text and scroll along until you run into code blocks.\n",
    "* Code blocks have computer code inside them ‚Äî hover over the block and you can run the code.\n",
    "* Run the code by hitting the ‚ñ∂Ô∏è \"play\" button to the left. If the code runs you'll see a ‚úîÔ∏è. If not, you'll get a ‚ùå.\n",
    "* The output and status of the code block will appear just below itself ‚Äî you need to scroll down further to see it.\n",
    "* Sometimes a code block will ask you for input in a hard-to-notice dialog box üëÜ at the top of your notebook window. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe IV. ü•ë Memories Maximized\n",
    "## üßë‚Äçüç≥ Cook well beyond the model's memory limits\n",
    "\n",
    "\n",
    "The length of a prompt is dependent upon the LLM you are using. Newer models can take more longer prompts; older models can only take shorter prompts. As a result, there's a limitation to how much context you can provide within any given prompt. \n",
    "\n",
    "| Model | Maximum Tokens** |\n",
    "|---|---|\n",
    "| ada | 2049 |\n",
    "| babbage | 2049 |\n",
    "| curie-001 | 2049 |\n",
    "| davinci-003 | 4097 |\n",
    "| GPT-4 | 8192 |\n",
    "\n",
    "** _1 token is approximately 3 characters; 1 page of book is roughly 500 tokens_\n",
    "\n",
    "A method that is growing in popularity is to use what are called \"embeddings\" ‚Äî which are high-dimensional numerical representations of any given text. It's possible to generate an \"embedding\" for a short piece of text or a longer piece of text. The length of the text is limited by the specific embedding model that you use.\n",
    "\n",
    "When using OpenAI or Azure OpenAI Service models, the `ada` model is both an inexpensive and good-enough choice for most use cases. Let's start our learnings with generating some embeddings, and see how they work in practice.\n",
    "\n",
    "## Step 1. Instantiate a üî• kernel for both completions and generating embeddings\n",
    "\n",
    "Note that the code below includes a few new lines that should be unfamiliar to you. They refer to using the `text-embedding-ada-002` model to use for generating the vector of numbers for a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: semantic-kernel==0.3.3.dev in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (0.3.3.dev0)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from semantic-kernel==0.3.3.dev) (23.1.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.2 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from semantic-kernel==0.3.3.dev) (1.24.2)\n",
      "Requirement already satisfied: openai<0.28.0,>=0.27.0 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from semantic-kernel==0.3.3.dev) (0.27.2)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from semantic-kernel==0.3.3.dev) (1.0.0)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from semantic-kernel==0.3.3.dev) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (2.28.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\achao\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.3.dev) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install semantic-kernel==0.3.3.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': <semantic_kernel.orchestration.sk_function.SKFunction at 0x1da47036740>,\n",
       " 'save': <semantic_kernel.orchestration.sk_function.SKFunction at 0x1da47036890>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, AzureTextEmbedding\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "if useAzureOpenAI:\n",
    "    api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureTextCompletion(\"text-davinci-003\", api_key, endpoint))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", AzureTextEmbedding(\"text-embedding-ada-002\", api_key, endpoint))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAITextCompletion(\"text-davinci-003\", api_key, org_id))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key, org_id))\n",
    "\n",
    "kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Add ü•ë memories to let the üî• kernel cook richer meals\n",
    "\n",
    "Imagine a collection of facts collected about you on the Internet as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four GIGANTIC vectors were generated just now from those 4 pieces of text above.\n"
     ]
    }
   ],
   "source": [
    "memoryCollectionName = \"Facts About Me\"\n",
    "\n",
    "await kernel.memory.save_information_async(memoryCollectionName, id=\"LinkedIn Bio\",\n",
    "    text=\"I currently work in the hotel industry at the front desk. I won the best team player award.\")\n",
    "\n",
    "await kernel.memory.save_information_async(memoryCollectionName, id=\"LinkedIn History\", \n",
    "    text=\"I have worked as a tourist operator for 8 years. I have also worked as a banking associate for 3 years.\");\n",
    "\n",
    "await kernel.memory.save_information_async(memoryCollectionName, id=\"Recent Facebook Post\", \n",
    "    text=\"My new dog Trixie is the cutest thing you've ever seen. She's just 2 years old.\");\n",
    "    \n",
    "await kernel.memory.save_information_async(memoryCollectionName, id=\"Old Facebook Post\", \n",
    "    text=\"Can you believe the size of the trees in Yellowstone? They're huge! I'm so committed to forestry concerns.\");\n",
    "\n",
    "print(\"Four GIGANTIC vectors were generated just now from those 4 pieces of text above.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚úÖ You'll need to have access to the `text-embedding-ada-002` model for the above to run correctly. Note that the Step 1 for this unit is different than all the other notebooks because it has this extra requirement to run.\n",
    "\n",
    "Next, imagine that you wanted to ask your LLM a question about you. What would it do? Well, given that it doesn't know anything about you out-of-the-box, it will simply make things up about you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm a creative problem solver with years of experience in design, marketing, and project management.\n"
     ]
    }
   ],
   "source": [
    "myFunction = kernel.create_semantic_function(\"\"\"\n",
    "Tell me about me and {{$input}} in less than 70 characters.\n",
    "\"\"\", max_tokens=100, temperature=0.8, top_p=1);\n",
    "result = await myFunction.invoke_async(input=\"my work history\");\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the semantic function above might say:\n",
    "\n",
    "`You are a creative problem solver with a varied work history.`\n",
    "\n",
    "That could apply to anybody, of course :+).\n",
    "\n",
    "Instead of hoping that the LLM comes up with the more correct answer, we can use memories to craft a more accurate completion. We do that by finding the most similar memories on file by searching through the memories we've stored, giving it the maximum number of hits we want back with `limit`, and set a threshold for how relevant we want a search to come back with `min_relevance_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "  >> LinkedIn History\n",
      "  Text: I have worked as a tourist operator for 8 years. I have also worked as a banking associate for 3 years.  Relevance: 0.8251813632095589\n",
      "\n",
      "Result 2:\n",
      "  >> LinkedIn Bio\n",
      "  Text: I currently work in the hotel industry at the front desk. I won the best team player award.  Relevance: 0.8024715127439459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask = \"Tell me about me and my work history.\"\n",
    "related_memory = \"I know nothing.\"\n",
    "counter = 0\n",
    "\n",
    "memories = await kernel.memory.search_async(memoryCollectionName, ask, limit=5, min_relevance_score=0.77)\n",
    "\n",
    "for memory in memories:\n",
    "    if counter == 0:\n",
    "        related_memory = memory.text\n",
    "    counter += 1\n",
    "    print(f\"Result {counter}:\\n  >> {memory.id}\\n  Text: {memory.text}  Relevance: {memory.relevance}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask the same question but with the most relevant context that we stored in `relatedMemory` to give to the LLM to come up with a more accurate response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 yrs tourist operator, 3 yrs banking associate.\n"
     ]
    }
   ],
   "source": [
    "myFunction = kernel.create_semantic_function(\"\"\"\n",
    "{{$input}}\n",
    "Tell me about me and my work history in less than 70 characters.\n",
    "\"\"\", max_tokens=100, temperature=0.1, top_p=0.1)\n",
    "\n",
    "result = await myFunction.invoke_async(input=related_memory)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feel the \"a-ha\" moment.\n",
    "\n",
    "### Manipulating ü•ë memories is how the token window limitation is addressed.\n",
    "\n",
    "Recall the table showing the maximum tokens that can be used per model:\n",
    "\n",
    "| Model | Maximum Tokens** |\n",
    "|---|---|\n",
    "| ada | 2049 |\n",
    "| babbage | 2049 |\n",
    "| curie-001 | 2049 |\n",
    "| davinci-003 | 4097 |\n",
    "| GPT-4 | 8192 |\n",
    "\n",
    "** _1 token is approximately 3 characters; 1 page of book is roughly 500 tokens_\n",
    "\n",
    "Given this same basic technique of gathering the most similar memories that are appropriate to a prompt, it's possible to have many more memories stored and available on-hand to compare with a given prompt. And it's not necessary to include just the top hit, but also more hits that are just as similar to the \"most relevant\" memory available. \n",
    "\n",
    "This is how an entire book can be used by Semantic Kernel as a memory source to feed into a prompt by only selecting the relevant chunks of text ‚Äî i.e. that which relates to the prompt. To do so you would:\n",
    "\n",
    "1. Generate embeddings for each of the paragraphs in the book.\n",
    "2. For a given prompt, find the most similar paragraphs within the book.\n",
    "3. Staying within the limitation of the token size window, gather all the related paragraphs.\n",
    "4. You now have a prompt with a great deal of relevant ü•ë context to send to the model.\n",
    "5. Reap the benefits of an \"informed\" LLM AI weighing in on a particular subject for you.\n",
    "\n",
    "Let's review this in practice. Say I have a 500-page book. \n",
    "\n",
    "1. I take each page and generate the embedding with `memory.save_information_async`\n",
    "2. I then take my prompt, `the best scenes are ones with flowers in it and deserve to be summarized` and use `memory.search_async` to locate the pages with flower scenes in them.\n",
    "3. Let's say there are three pages that are relevant. Those three pages will be used to compose a new prompt that's simply the three pages appended to each other along with the original prompt. If instead you need to include ten pages, and exceed the token window, then summarize each of the ten pages separately into ten shorter passages. Do this until you meet the token window requirements.\n",
    "4. You have the prompt to give to the model you've chosen. It has pulled the relevant information out of the 500-page book, and will do its best to summarize what you care about the most.\n",
    "5. Ta-da! You'll get what you've asked for."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this point, we can take Abraham Lincoln's famous Gettysburg Address and use it to generate a new speech. We'll use the OpenAI API to generate the speech, and then we'll use the Azure Cognitive Search API to search for the speech in the text of the Gettysburg Address and break it up into chunks of text and then processed with embeddings. We've asked the GPT-3 model to write a simple text chunking procedure where a specified maximum length of a chunk is given. Chunking is still more of an art than a science, so you can see the result isn't as perfect as we'd like. But this will give you a sense of how a large text file can be processed into smaller pieces of text that get used by an LLM AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def chunk_text_file(file_path: str, recommended_length: int) -> List[str]:\n",
    "    chunks = []\n",
    "\n",
    "    # Read in the text file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Break the text into chunks of the recommended length\n",
    "    start_index = 0\n",
    "    while start_index < len(text):\n",
    "        end_index = start_index + recommended_length\n",
    "        if end_index > len(text):\n",
    "            end_index = len(text)\n",
    "\n",
    "        # Look for a natural breakage point like a paragraph or just before a new heading\n",
    "        while end_index < len(text) and not text[end_index].isspace():\n",
    "            end_index += 1\n",
    "\n",
    "        # Get the chunk of text\n",
    "        chunk = text[start_index:end_index]\n",
    "\n",
    "        # Strip the whitespace at the start and end of the string\n",
    "        chunk = chunk.strip()\n",
    "\n",
    "        # Add the chunk to the list\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Move to the next chunk\n",
    "        start_index = end_index\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: Four score and seven years ago our fathers brought forth upon this continent a new nation, conceived in liberty, and dedicated to the proposition\n",
      "Chunk 1: that all men are created equal. (Applause.) Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived\n",
      "Chunk 2: and so dedicated, can long endure. We are met on a great battle field of that war; we are met to dedicate a portion of it as the final resting\n",
      "Chunk 3: place of those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this, but in a larger\n",
      "Chunk 4: sense, we cannot dedicate, we cannot consecrate, we cannot hallow this ground.\n",
      "The brave men, living and dead, who struggled here have consecrated\n",
      "Chunk 5: it far above our poor power to add or to detract. (Applause.) The world will little note, nor long remember, what we say here; but it can never\n",
      "Chunk 6: forget what they did here. (Applause.) It is for us, the living, rather to be dedicated here to the unfinished work that they have thus far\n",
      "Chunk 7: so nobly carried on. (Applause.) It is rather for us here to be dedicated to the great task remaining before us; that from these honored dead\n",
      "Chunk 8: we take increased devotion to that cause for which they gave the last full measure of devotion; that we here highly resolve that the dead shall\n",
      "Chunk 9: not have died in vain. (Applause.) That the nation shall, under God, have a new birth of freedom, and that the government of the people, by\n",
      "Chunk 10: the people and for the people, shall not perish from the earth. (Long applause.)\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_text_file(\"./lincoln.txt\", 140)\n",
    "lincolnMemoryCollectionName = \"Abe's Words\"\n",
    "counter = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk {counter}: {chunk}\")\n",
    "    await kernel.memory.save_information_async(lincolnMemoryCollectionName, id=f\"Chunk {counter}\", text=chunk)\n",
    "    counter += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query these chunks for the most similar ones that match a simple question: `\"What should the people do?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "  >> Chunk 10\n",
      "  Text: the people and for the people, shall not perish from the earth. (Long applause.)  Relevance: 0.8016110378803689\n",
      "\n",
      "Result 2:\n",
      "  >> Chunk 6\n",
      "  Text: forget what they did here. (Applause.) It is for us, the living, rather to be dedicated here to the unfinished work that they have thus far  Relevance: 0.7707327774000975\n",
      "\n",
      "Memory to feed back into the prompt will be:\n",
      "  >> the people and for the people, shall not perish from the earth. (Long applause.) forget what they did here. (Applause.) It is for us, the living, rather to be dedicated here to the unfinished work that they have thus far \n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "myPrompt = \"What should the people do?\"\n",
    "myMemory = \"\"\n",
    "memories = await kernel.memory.search_async(lincolnMemoryCollectionName,\n",
    "             myPrompt, limit=5, min_relevance_score=0.77)\n",
    "\n",
    "for memory in memories:\n",
    "    counter += 1\n",
    "    print(f\"Result {counter}:\\n  >> {memory.id}\\n  Text: {memory.text}  Relevance: {memory.relevance}\\n\")\n",
    "    myMemory += memory.text + \" \"\n",
    "\n",
    "print(\"Memory to feed back into the prompt will be:\\n  >> \" + myMemory+ \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response ... 'according to Lincoln':\n",
      "\n",
      "The people should continue the work that those before them have started, and strive to make the world a better place for all. They should work together to ensure that the values of freedom, justice, and equality are upheld and that the rights of all people are respected.\n"
     ]
    }
   ],
   "source": [
    "myLincolnFunction = kernel.create_semantic_function(\"\"\"\n",
    "Lincoln said:\n",
    "---\n",
    "{{$input}}\n",
    "---\n",
    "So what should the people do?\n",
    "\"\"\", max_tokens=100, temperature=0.1, top_p=0.1)\n",
    "\n",
    "lincolnResult = await myLincolnFunction.invoke_async(input=myMemory)\n",
    "\n",
    "print(\"Generated response ... 'according to Lincoln':\\n\" + str(lincolnResult))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this run at scale, check out the GitHub Q&A Sample app available at [https://aka.ms/sk/repo](https://aka.ms/sk/repo). It takes an entire code repo, converts it to embeddings, and it lets you \"chat\" with the repo itself. Keep in mind that it would be generally impossible to feed the entire repo into an LLM AI's window, and that's where using ü•ë memories come in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è≠Ô∏è Next Steps\n",
    "\n",
    "Run through more advanced examples in the notebooks that are available in our GitHub repo at [https://aka.ms/sk/repo](https://aka.ms/sk/repo).\n",
    "\n",
    "[Learn about üçã connectors!](../e5-connectors/notebook.ipynb)\n",
    "\n",
    "Or stay a longer while and add more facts about yourself in the `MemoryCollection`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
